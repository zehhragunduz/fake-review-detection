import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 1. KaydedilmiÅŸ modeli ve tokenizer'Ä± yÃ¼kle
model_path = "./saved_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# 2. Yorumu iÅŸle ve tahmin et
def predict(text):
    encoding = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=128
    )

    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.softmax(outputs.logits, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        confidence = probs[0][pred].item()

    label = "âœ… GERÃ‡EK" if pred == 1 else "âŒ SAHTE"
    print(f"\nğŸ“ Yorum: {text}")
    print(f"{label} yorum (%{confidence * 100:.2f} gÃ¼ven)\n" + "-"*60)

# 3. KullanÄ±cÄ±dan yorum al
print("ğŸ“¢ BERTurk Yorum SÄ±nÄ±flandÄ±rÄ±cÄ±ya HoÅŸgeldin!\nÃ‡Ä±kmak iÃ§in 'q' yaz.\n")

while True:
    user_input = input("Yorum girin: ")
    if user_input.lower() == "q":
        print("Ã‡Ä±kÄ±lÄ±yor... ğŸ’¨")
        break
    predict(user_input)
